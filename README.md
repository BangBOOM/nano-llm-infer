# NANO-LLM-Inference

Development efficiency is the first citizen in this project

## v0.0.1
no kvcache, recompute during each inference step

process one batch a time

## TODO
1. PageAttention KVCache Support
2. Batch Prefill
3. Distributed Inference with TP
4. Distributed Inference with EP
